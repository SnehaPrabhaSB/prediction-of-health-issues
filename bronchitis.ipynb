{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# Any results you write to the current directory are saved as output.\n# 176_1b4_Al_mc_AKGC417L.txt","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.listdir('../input/respiratory-sound-database/respiratory_sound_database/Respiratory_Sound_Database')","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"['patient_diagnosis.csv',\n 'filename_format.txt',\n 'audio_and_txt_files',\n 'filename_differences.txt']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \\\n'../input/respiratory-sound-database/respiratory_sound_database/Respiratory_Sound_Database/filename_differences.txt'\n\ndf_diff = pd.read_csv(path, sep=\" \", header=None, names=['file_names'])\n\ndf_diff.head(10)","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                 file_names\n0  '101_1b1_Al_sc_AKGC417L'\n1  '101_1b1_Pr_sc_AKGC417L'\n2  '102_1b1_Ar_sc_AKGC417L'\n3  '105_1b1_Tc_sc_LittC2SE'\n4  '108_1b1_Al_sc_LittC2SE'\n5  '111_1b2_Tc_sc_LittC2SE'\n6  '111_1b3_Tc_sc_LittC2SE'\n7  '115_1b1_Ar_sc_LittC2SE'\n8  '116_1b2_Pl_sc_LittC2SE'\n9  '116_1b2_Tc_sc_LittC2SE'","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_names</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>'101_1b1_Al_sc_AKGC417L'</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>'101_1b1_Pr_sc_AKGC417L'</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>'102_1b1_Ar_sc_AKGC417L'</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>'105_1b1_Tc_sc_LittC2SE'</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>'108_1b1_Al_sc_LittC2SE'</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>'111_1b2_Tc_sc_LittC2SE'</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>'111_1b3_Tc_sc_LittC2SE'</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>'115_1b1_Ar_sc_LittC2SE'</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>'116_1b2_Pl_sc_LittC2SE'</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>'116_1b2_Tc_sc_LittC2SE'</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wave\nimport math\nimport scipy.io.wavfile as wf\n#wave file reader\n\n#Will resample all files to the target sample rate and produce a 32bit float array\ndef read_wav_file(str_filename, target_rate):\n    wav = wave.open(str_filename, mode = 'r')\n    (sample_rate, data) = extract2FloatArr(wav,str_filename)\n    \n    if (sample_rate != target_rate):\n        ( _ , data) = resample(sample_rate, data, target_rate)\n        \n    wav.close()\n    return (target_rate, data.astype(np.float32))\n\ndef resample(current_rate, data, target_rate):\n    x_original = np.linspace(0,100,len(data))\n    x_resampled = np.linspace(0,100, int(len(data) * (target_rate / current_rate)))\n    resampled = np.interp(x_resampled, x_original, data)\n    return (target_rate, resampled.astype(np.float32))\n\n# -> (sample_rate, data)\ndef extract2FloatArr(lp_wave, str_filename):\n    (bps, channels) = bitrate_channels(lp_wave)\n    \n    if bps in [1,2,4]:\n        (rate, data) = wf.read(str_filename)\n        divisor_dict = {1:255, 2:32768}\n        if bps in [1,2]:\n            divisor = divisor_dict[bps]\n            data = np.divide(data, float(divisor)) #clamp to [0.0,1.0]        \n        return (rate, data)\n    \n    elif bps == 3: \n        #24bpp wave\n        return read24bitwave(lp_wave)\n    \n    else:\n        raise Exception('Unrecognized wave format: {} bytes per sample'.format(bps))\n        \n#Note: This function truncates the 24 bit samples to 16 bits of precision\n#Reads a wave object returned by the wave.read() method\n#Returns the sample rate, as well as the audio in the form of a 32 bit float numpy array\n#(sample_rate:float, audio_data: float[])\ndef read24bitwave(lp_wave):\n    nFrames = lp_wave.getnframes()\n    buf = lp_wave.readframes(nFrames)\n    reshaped = np.frombuffer(buf, np.int8).reshape(nFrames,-1)\n    short_output = np.empty((nFrames, 2), dtype = np.int8)\n    short_output[:,:] = reshaped[:, -2:]\n    short_output = short_output.view(np.int16)\n    return (lp_wave.getframerate(), np.divide(short_output, 32768).reshape(-1))  #return numpy array to save memory via array slicing\n\ndef bitrate_channels(lp_wave):\n    bps = (lp_wave.getsampwidth() / lp_wave.getnchannels()) #bytes per sample\n    return (bps, lp_wave.getnchannels())\n\ndef slice_data(start, end, raw_data,  sample_rate):\n    max_ind = len(raw_data) \n    start_ind = min(int(start * sample_rate), max_ind)\n    end_ind = min(int(end * sample_rate), max_ind)\n    return raw_data[start_ind: end_ind]\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nroot = '../input/respiratory-sound-database/respiratory_sound_database/Respiratory_Sound_Database/audio_and_txt_files/'\nfilenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Extract_Annotation_Data(file_name, root):\n    tokens = file_name.split('_')\n    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n    return (recording_info, recording_annotations)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_list = []\nrec_annotations = []\nrec_annotations_dict = {}\nfor s in filenames:\n    (i,a) = Extract_Annotation_Data(s, root)\n    i_list.append(i)\n    rec_annotations.append(a)\n    rec_annotations_dict[s] = a\nrecording_info = pd.concat(i_list, axis = 0)\nrecording_info.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"  Patient number Recording index Chest location Acquisition mode  \\\n0            168             1b1             Al               sc   \n0            172             1b4             Ar               mc   \n0            130             3p2             Pr               mc   \n0            177             2b4             Pl               mc   \n0            130             2p5             Pl               mc   \n\n  Recording equipment  \n0            Meditron  \n0            AKGC417L  \n0            AKGC417L  \n0            AKGC417L  \n0            AKGC417L  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient number</th>\n      <th>Recording index</th>\n      <th>Chest location</th>\n      <th>Acquisition mode</th>\n      <th>Recording equipment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>168</td>\n      <td>1b1</td>\n      <td>Al</td>\n      <td>sc</td>\n      <td>Meditron</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>172</td>\n      <td>1b4</td>\n      <td>Ar</td>\n      <td>mc</td>\n      <td>AKGC417L</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>130</td>\n      <td>3p2</td>\n      <td>Pr</td>\n      <td>mc</td>\n      <td>AKGC417L</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>177</td>\n      <td>2b4</td>\n      <td>Pl</td>\n      <td>mc</td>\n      <td>AKGC417L</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>130</td>\n      <td>2p5</td>\n      <td>Pl</td>\n      <td>mc</td>\n      <td>AKGC417L</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_label_list = []\ncrack_list = []\nwheeze_list = []\nboth_sym_list = []\nfilename_list = []\nfor f in filenames:\n    d = rec_annotations_dict[f]\n    no_labels = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 0)].index)\n    n_crackles = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 0)].index)\n    n_wheezes = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 1)].index)\n    both_sym = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 1)].index)\n    no_label_list.append(no_labels)\n    crack_list.append(n_crackles)\n    wheeze_list.append(n_wheezes)\n    both_sym_list.append(both_sym)\n    filename_list.append(f)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duration_list = []\nfor i in range(len(rec_annotations)):\n    current = rec_annotations[i]\n    duration = current['End'] - current['Start']\n    duration_list.extend(duration)\n\nduration_list = np.array(duration_list)\nplt.hist(duration_list, bins = 50)\nprint('longest cycle:{}'.format(max(duration_list)))\nprint('shortest cycle:{}'.format(min(duration_list)))\nthreshold = 5\nprint('Fraction of samples less than {} seconds:{}'.format(threshold,\n                                                           np.sum(duration_list < threshold)/len(duration_list)))","execution_count":9,"outputs":[{"output_type":"stream","text":"longest cycle:16.163\nshortest cycle:0.20000000000000284\nFraction of samples less than 5 seconds:0.9660771238040011\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.signal\n\n#vtlp_params = (alpha, f_high) \ndef sample2MelSpectrum(cycle_info, sample_rate, n_filters, vtlp_params):\n    n_rows = 175 # 7500 cutoff\n    n_window = 512 #~25 ms window\n    (f, t, Sxx) = scipy.signal.spectrogram(cycle_info[0],fs = sample_rate, nfft= n_window, nperseg=n_window)\n    Sxx = Sxx[:n_rows,:].astype(np.float32) #sift out coefficients above 7500hz, Sxx has 196 columns\n    mel_log = FFT2MelSpectrogram(f[:n_rows], Sxx, sample_rate, n_filters, vtlp_params)[1]\n    mel_min = np.min(mel_log)\n    mel_max = np.max(mel_log)\n    diff = mel_max - mel_min\n    norm_mel_log = (mel_log - mel_min) / diff if (diff > 0) else np.zeros(shape = (n_filters,Sxx.shape[1]))\n    if (diff == 0):\n        print('Error: sample data is completely empty')\n    labels = [cycle_info[1], cycle_info[2]] #crackles, wheezes flags\n    return (np.reshape(norm_mel_log, (n_filters,Sxx.shape[1],1)).astype(np.float32), # 196x64x1 matrix\n            label2onehot(labels)) \n        \ndef Freq2Mel(freq):\n    return 1125 * np.log(1 + freq / 700)\n\ndef Mel2Freq(mel):\n    exponents = mel / 1125\n    return 700 * (np.exp(exponents) - 1)\n\n#Tased on Jaitly & Hinton(2013)\n#Takes an array of the original mel spaced frequencies and returns a warped version of them\ndef VTLP_shift(mel_freq, alpha, f_high, sample_rate):\n    nyquist_f = sample_rate / 2\n    warp_factor = min(alpha, 1)\n    threshold_freq = f_high * warp_factor / alpha\n    lower = mel_freq * alpha\n    higher = nyquist_f - (nyquist_f - mel_freq) * ((nyquist_f - f_high * warp_factor) / (nyquist_f - f_high * (warp_factor / alpha)))\n    \n    warped_mel = np.where(mel_freq <= threshold_freq, lower, higher)\n    return warped_mel.astype(np.float32)\n\n#mel_space_freq: the mel frequencies (HZ) of the filter banks, in addition to the two maximum and minimum frequency values\n#fft_bin_frequencies: the bin freqencies of the FFT output\n#Generates a 2d numpy array, with each row containing each filter bank\ndef GenerateMelFilterBanks(mel_space_freq, fft_bin_frequencies):\n    n_filters = len(mel_space_freq) - 2\n    coeff = []\n    #Triangular filter windows\n    #ripped from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n    for mel_index in range(n_filters):\n        m = int(mel_index + 1)\n        filter_bank = []\n        for f in fft_bin_frequencies:\n            if(f < mel_space_freq[m-1]):\n                hm = 0\n            elif(f < mel_space_freq[m]):\n                hm = (f - mel_space_freq[m-1]) / (mel_space_freq[m] - mel_space_freq[m-1])\n            elif(f < mel_space_freq[m + 1]):\n                hm = (mel_space_freq[m+1] - f) / (mel_space_freq[m + 1] - mel_space_freq[m])\n            else:\n                hm = 0\n            filter_bank.append(hm)\n        coeff.append(filter_bank)\n    return np.array(coeff, dtype = np.float32)\n        \n#Transform spectrogram into mel spectrogram -> (frequencies, spectrum)\n#vtlp_params = (alpha, f_high), vtlp will not be applied if set to None\ndef FFT2MelSpectrogram(f, Sxx, sample_rate, n_filterbanks, vtlp_params = None):\n    (max_mel, min_mel)  = (Freq2Mel(max(f)), Freq2Mel(min(f)))\n    mel_bins = np.linspace(min_mel, max_mel, num = (n_filterbanks + 2))\n    #Convert mel_bins to corresponding frequencies in hz\n    mel_freq = Mel2Freq(mel_bins)\n    \n    if(vtlp_params is None):\n        filter_banks = GenerateMelFilterBanks(mel_freq, f)\n    else:\n        #Apply VTLP\n        (alpha, f_high) = vtlp_params\n        warped_mel = VTLP_shift(mel_freq, alpha, f_high, sample_rate)\n        filter_banks = GenerateMelFilterBanks(warped_mel, f)\n        \n    mel_spectrum = np.matmul(filter_banks, Sxx)\n    return (mel_freq[1:-1], np.log10(mel_spectrum  + float(10e-12)))\n    \n#labels proved too difficult to train (model keep convergining to statistical mean)\n#Flattened to onehot labels since the number of combinations is very low\ndef label2onehot(c_w_flags):\n    c = c_w_flags[0]\n    w = c_w_flags[1]\n    if((c == False) & (w == False)):\n        return [1,0,0,0]\n    elif((c == True) & (w == False)):\n        return [0,1,0,0]\n    elif((c == False) & (w == True)):\n        return [0,0,1,0]\n    else:\n        return [0,0,0,1]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Used to split each individual sound file into separate sound clips containing one respiratory cycle each\n#output: [filename, (sample_data:np.array, start:float, end:float, crackles:bool(float), wheezes:bool(float)) (...) ]\ndef get_sound_samples(recording_annotations, file_name, root, sample_rate):\n    sample_data = [file_name]\n    (rate, data) = read_wav_file(os.path.join(root, file_name + '.wav'), sample_rate)\n    \n    for i in range(len(recording_annotations.index)):\n        row = recording_annotations.loc[i]\n        start = row['Start']\n        end = row['End']\n        crackles = row['Crackles']\n        wheezes = row['Wheezes']\n        audio_chunk = slice_data(start, end, data, rate)\n        sample_data.append((audio_chunk, start,end,crackles,wheezes))\n    return sample_data\n\n#Fits each respiratory cycle into a fixed length audio clip, splits may be performed and zero padding is added if necessary\n#original:(arr,c,w) -> output:[(arr,c,w),(arr,c,w)]\ndef split_and_pad(original, desiredLength, sampleRate):\n    output_buffer_length = int(desiredLength * sampleRate)\n    soundclip = original[0]\n    n_samples = len(soundclip)\n    total_length = n_samples / sampleRate #length of cycle in seconds\n    n_slices = int(math.ceil(total_length / desiredLength)) #get the minimum number of slices needed\n    samples_per_slice = n_samples // n_slices\n    src_start = 0 #Staring index of the samples to copy from the original buffer\n    output = [] #Holds the resultant slices\n    for i in range(n_slices):\n        src_end = min(src_start + samples_per_slice, n_samples)\n        length = src_end - src_start\n        copy = generate_padded_samples(soundclip[src_start:src_end], output_buffer_length)\n        output.append((copy, original[1], original[2]))\n        src_start += length\n    return output\n\ndef generate_padded_samples(source, output_length):\n    copy = np.zeros(output_length, dtype = np.float32)\n    src_length = len(source)\n    frac = src_length / output_length\n    if(frac < 0.5):\n        #tile forward sounds to fill empty space\n        cursor = 0\n        while(cursor + src_length) < output_length:\n            copy[cursor:(cursor + src_length)] = source[:]\n            cursor += src_length\n    else:\n        copy[:src_length] = source[:]\n    #\n    return copy","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creates a copy of each time slice, but stretches or contracts it by a random amount\ndef gen_time_stretch(original, sample_rate, max_percent_change):\n    stretch_amount = 1 + np.random.uniform(-1,1) * (max_percent_change / 100)\n    (_, stretched) = resample(sample_rate, original, int(sample_rate * stretch_amount)) \n    return stretched\n\n#Same as above, but applies it to a list of samples\ndef augment_list(audio_with_labels, sample_rate, percent_change, n_repeats):\n    augmented_samples = []\n    for i in range(n_repeats):\n        addition = [(gen_time_stretch(t[0], sample_rate, percent_change), t[1], t[2] ) for t in audio_with_labels]\n        augmented_samples.extend(addition)\n    return augmented_samples\n\n#Takes a list of respiratory cycles, and splits and pads each cycle into fixed length buffers (determined by desiredLength(seconds))\n#Then takes the split and padded sample and transforms it into a mel spectrogram\n#VTLP_alpha_range = [Lower, Upper] (Bounds of random selection range), \n#VTLP_high_freq_range = [Lower, Upper] (-)\n#output:[(arr:float[],c:float_bool,w:float_bool),(arr,c,w)]\ndef split_and_pad_and_apply_mel_spect(original, desiredLength, sampleRate, VTLP_alpha_range = None, VTLP_high_freq_range = None, n_repeats = 1, dimension=13):\n    output = []\n    for i in range(n_repeats):\n        for d in original:\n            lst_result = split_and_pad(d, desiredLength, sampleRate) #Time domain\n            if( (VTLP_alpha_range is None) | (VTLP_high_freq_range is None) ):\n                #Do not apply VTLP\n                VTLP_params = None\n            else:\n                #Randomly generate VLTP parameters\n                alpha = np.random.uniform(VTLP_alpha_range[0], VTLP_alpha_range[1])\n                high_freq = np.random.uniform(VTLP_high_freq_range[0], VTLP_high_freq_range[1])\n                VTLP_params = (alpha, high_freq)\n            freq_result = [sample2MelSpectrum(d, sampleRate, dimension, VTLP_params) for d in lst_result] #Freq domain\n            output.extend(freq_result)\n    return output","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_all_training_samples(filenames, annotation_dict, root, target_rate, desired_length):\n    cycle_list = []\n    for file in filenames:\n        data = get_sound_samples(annotation_dict[file], file, root, target_rate)\n        cycles_with_labels = [(d[0], d[3], d[4]) for d in data[1:]]\n        cycle_list.extend(cycles_with_labels)\n    \n    #Sort into respective classes\n    no_labels = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 0))]\n    c_only = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 0))] \n    w_only = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 1))]\n    c_w = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 1))]\n    \n    #Count of labels across all cycles, actual recording time also follows similar ratios\n    #none:3642\n    #crackles:1864 \n    #wheezes:886\n    #both:506\n    none_train = no_labels\n    c_train = c_only\n    w_train = w_only\n    c_w_train = c_w\n    \n    #Training section (Data augmentation procedures)\n    #Augment w_only and c_w groups to match the size of c_only\n    #no_labels will be artifically reduced in the pipeline  later\n    w_stretch = w_train + augment_list(w_train, target_rate, 10 , 1) #\n    c_w_stretch = c_w_train + augment_list(c_w_train , target_rate, 10 , 1) \n    \n    #Split up cycles into sound clips with fixed lengths so they can be fed into a CNN\n    vtlp_alpha = [0.9,1.1]\n    vtlp_upper_freq = [3200,3800]\n    \n    train_none  = (split_and_pad_and_apply_mel_spect(none_train, desired_length, target_rate) +\n                   split_and_pad_and_apply_mel_spect(none_train, desired_length, target_rate, vtlp_alpha))\n    \n    train_c = (split_and_pad_and_apply_mel_spect(c_train, desired_length, target_rate) + \n               split_and_pad_and_apply_mel_spect(c_train, desired_length, target_rate, vtlp_alpha, vtlp_upper_freq, n_repeats = 3) ) #original samples + VTLP\n    \n    train_w = (split_and_pad_and_apply_mel_spect(w_stretch, desired_length, target_rate) + \n               split_and_pad_and_apply_mel_spect(w_stretch , desired_length, target_rate, vtlp_alpha , vtlp_upper_freq, n_repeats = 4)) #(original samples + time stretch) + VTLP\n    \n    train_c_w = (split_and_pad_and_apply_mel_spect(c_w_stretch, desired_length, target_rate) + \n                 split_and_pad_and_apply_mel_spect(c_w_stretch, desired_length, target_rate, vtlp_alpha , vtlp_upper_freq, n_repeats = 7)) #(original samples + time stretch * 2) + VTLP\n    \n    train_dict = {'none':train_none,'crackles':train_c,'wheezes':train_w, 'both':train_c_w}\n    \n    \n    return train_dict\n\n\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_sample_rate = 22000 \nsample_length_seconds = 5\nsample_dict = extract_all_training_samples(filenames, rec_annotations_dict, root, target_sample_rate, sample_length_seconds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_clips = sample_dict\nsample_height = training_clips['none'][0][0].shape[0]\nsample_width = training_clips['none'][0][0].shape[1]\nind = 1\nplt.figure(figsize = (10,10))\nplt.subplot(4,1,1)\nplt.imshow(training_clips['none'][ind][0].reshape(sample_height, sample_width))\nplt.title('None')\nplt.subplot(4,1,2)\nplt.imshow(training_clips['crackles'][ind][0].reshape(sample_height, sample_width))\nplt.title('Crackles')\nplt.subplot(4,1,3)\nplt.imshow(training_clips['wheezes'][ind][0].reshape(sample_height, sample_width))\nplt.title('Wheezes')\nplt.subplot(4,1,4)\nplt.imshow(training_clips['both'][ind][0].reshape(sample_height, sample_width))\nplt.title('Both')\nplt.tight_layout()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}